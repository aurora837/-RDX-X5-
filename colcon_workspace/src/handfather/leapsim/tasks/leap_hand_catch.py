

import os
import sys
from attr import has
from importlib_metadata import itertools
import torch
import numpy as np
from isaacgym import gymtorch
from isaacgym import gymapi, gymutil
from isaacgym.torch_utils import quat_conjugate, quat_mul, quat_rotate, to_torch, unscale, quat_apply, tensor_clamp, torch_rand_float, scale
from glob import glob
import math
import torchvision
import warnings
import matplotlib.pyplot as plt
from .base.vec_task import VecTask
from collections import deque

class LeapHandCATCH(VecTask):
    def __init__(self, cfg, rl_device, sim_device, graphics_device_id, headless, virtual_screen_capture=None, force_render=None):
        self.cfg = cfg
        self.set_defaults()
        # before calling init in VecTask, need to do
        # 1. setup randomization
        self._setup_domain_rand_cfg(cfg['env']['randomization'])
        # 2. setup privileged information
        self._setup_priv_option_cfg(cfg['env']['privInfo'])
        # 3. setup object assets
        self._setup_object_info(cfg['env']['object'])
        # 4. setup reward
        self._setup_reward_cfg(cfg['env']['reward'])
        self.base_obj_scale = cfg['env']['baseObjScale']
        # ç§»é™¤ save_init_pose å’Œ grasp_cache_name - ä½¿ç”¨åŸºäºé˜¶æ®µçš„æŠ“å–ç­–ç•¥
        self.aggregate_mode = self.cfg['env']['aggregateMode']
        self.up_axis = 'z'
        self.reset_z_threshold = self.cfg['env']['reset_height_threshold']
        self.evaluate = self.cfg['on_evaluation']
        
        # åˆ†é˜¶æ®µæŠ“å–ç­–ç•¥é…ç½®
        self.approach_distance_threshold = self.cfg['env'].get('approach_distance_threshold', 0.15)
        self.contact_threshold = self.cfg['env'].get('contact_threshold', 2)
        self.lift_height_threshold = self.cfg['env'].get('lift_height_threshold', 0.05)

        super().__init__(cfg, rl_device, sim_device, graphics_device_id, headless)

        self.debug_viz = self.cfg['env']['enableDebugVis']
        self.max_episode_length = self.cfg['env']['episodeLength']
        self.dt = self.sim_params.dt
        self.control_dt = self.sim_params.dt * self.control_freq_inv # This is the actual control frequency

        if self.viewer:
            self.default_cam_pos = gymapi.Vec3(0.0, 0.4, 1.5)
            self.default_cam_target = gymapi.Vec3(0.0, 0.0, 0.5)
            self.gym.viewer_camera_look_at(self.viewer, None, self.default_cam_pos, self.default_cam_target)

        # get gym GPU state tensors
        actor_root_state_tensor = self.gym.acquire_actor_root_state_tensor(self.sim)
        dof_state_tensor = self.gym.acquire_dof_state_tensor(self.sim)
        dof_force_tensor = self.gym.acquire_dof_force_tensor(self.sim)
        rigid_body_tensor = self.gym.acquire_rigid_body_state_tensor(self.sim)
        net_contact_forces = self.gym.acquire_net_contact_force_tensor(self.sim)

        # create some wrapper tensors for different slices
        self.leap_hand_default_dof_pos = torch.zeros(self.num_leap_hand_dofs, dtype=torch.float, device=self.device)
        self.dof_state = gymtorch.wrap_tensor(dof_state_tensor)
        self.contact_forces = gymtorch.wrap_tensor(net_contact_forces).view(self.num_envs, -1, 3)
        self.leap_hand_dof_state = self.dof_state.view(self.num_envs, -1, 2)[:, :self.num_leap_hand_dofs]
        self.leap_hand_dof_pos = self.leap_hand_dof_state[..., 0]
        self.leap_hand_dof_vel = self.leap_hand_dof_state[..., 1]

        self.object_rpy = torch.zeros((self.num_envs, 3), device=self.device)
        self.object_angvel_finite_diff = torch.zeros((self.num_envs, 3), device=self.device)

        self.rigid_body_states = gymtorch.wrap_tensor(rigid_body_tensor).view(self.num_envs, -1, 13)
        self.num_bodies = self.rigid_body_states.shape[1]

        self.root_state_tensor = gymtorch.wrap_tensor(actor_root_state_tensor).view(-1, 13)
        self.torques = gymtorch.wrap_tensor(dof_force_tensor).view(-1, self.num_leap_hand_dofs)

        self.global_counter = 0
        self.prev_global_counter = 0

        self._refresh_gym()

        self.num_dofs = self.gym.get_sim_dof_count(self.sim) // self.num_envs

        self.prev_targets = torch.zeros((self.num_envs, self.num_dofs), dtype=torch.float, device=self.device)
        self.cur_targets = torch.zeros((self.num_envs, self.num_dofs), dtype=torch.float, device=self.device)
        # object apply random forces parameters
        self.force_scale = self.cfg['env'].get('forceScale', 0.0)
        self.random_force_prob_scalar = self.cfg['env'].get('randomForceProbScalar', 0.0)
        self.force_decay = self.cfg['env'].get('forceDecay', 0.99)
        self.force_decay_interval = self.cfg['env'].get('forceDecayInterval', 0.08)
        self.force_decay = to_torch(self.force_decay, dtype=torch.float, device=self.device)
        self.rb_forces = torch.zeros((self.num_envs, self.num_bodies, 3), dtype=torch.float, device=self.device)    
        self.early_termination_buf = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)

        # ç§»é™¤ç¼“å­˜åŠ è½½é€»è¾‘ - ç°åœ¨ä½¿ç”¨åŸºäºé˜¶æ®µçš„æŠ“å–ç­–ç•¥
        # ä¸å†éœ€è¦é¢„è®¡ç®—çš„æŠ“å–å§¿æ€ç¼“å­˜
        
        # åŸºäºé˜¶æ®µçš„æŠ“å–ä»»åŠ¡ä¸éœ€è¦æ—‹è½¬è½´ç¼“å†²åŒº
        # self.rot_axis_buf å·²ç§»é™¤ - ä¸“æ³¨äºæŠ“å–è€Œéæ—‹è½¬

        # useful buffers
        self.init_pose_buf = torch.zeros((self.num_envs, self.num_dofs), device=self.device, dtype=torch.float)
        self.object_init_pose_buf = torch.zeros((self.num_envs, 7), device=self.device, dtype=torch.float)
        # ç§»é™¤ previous_object_rot - é˜¶æ®µæŠ“å–ç³»ç»Ÿä¸éœ€è¦è·Ÿè¸ªç‰©ä½“æ—‹è½¬å˜åŒ–
        # self.previous_object_rot = torch.zeros((self.num_envs, 4), device=self.device)
        self.actions = torch.zeros((self.num_envs, self.num_actions), device=self.device, dtype=torch.float)
        self.last_torques = self.torques.clone()
        self.dof_vel_finite_diff = torch.zeros((self.num_envs, self.num_dofs), device=self.device, dtype=torch.float)
        assert type(self.p_gain) in [int, float] and type(self.d_gain) in [int, float], 'assume p_gain and d_gain are only scalars'
        self.p_gain = torch.ones((self.num_envs, self.num_actions), device=self.device, dtype=torch.float) * self.p_gain
        self.d_gain = torch.ones((self.num_envs, self.num_actions), device=self.device, dtype=torch.float) * self.d_gain
        self.resample_randomizations(None) 

        # debug and understanding statistics
        self.env_timeout_counter = to_torch(np.zeros((len(self.envs)))).long().to(self.device)  # max 10 (10000 envs)
        self.stat_sum_rewards = 0
        # ç§»é™¤æ—‹è½¬å¥–åŠ±ç»Ÿè®¡ - æ”¹ä¸ºæŠ“å–æˆåŠŸç‡ç»Ÿè®¡
        # self.stat_sum_rotate_rewards = 0  # ä¸å†éœ€è¦æ—‹è½¬å¥–åŠ±
        self.stat_sum_episode_length = 0
        self.stat_sum_obj_linvel = 0
        self.stat_sum_torques = 0
        self.env_evaluated = 0
        self.max_evaluate_envs = 500000
        # ä¿ç•™è§’é€Ÿåº¦ç¼“å†²åŒºç”¨äºç‰©ç†ç¨³å®šæ€§æ£€æŸ¥ï¼Œä½†ä¸ç”¨äºæ—‹è½¬å¥–åŠ±
        self.object_angvel_finite_diff_ep_buf = deque(maxlen=1000)
        self.object_angvel_finite_diff_mean = torch.zeros(self.num_envs, device=self.device)
        
        # åŸºäºé˜¶æ®µçš„æŠ“å–ç»Ÿè®¡
        self.stat_sum_approach_success = 0
        self.stat_sum_contact_success = 0
        self.stat_sum_lift_success = 0
        self.stat_sum_grasp_complete = 0
        
        # åŸºäºé˜¶æ®µçš„æŠ“å–çŠ¶æ€è¿½è¸ª
        # é˜¶æ®µ: 0=æ¥è¿‘ç‰©ä½“, 1=æ¥è§¦å¹¶é—­åˆæ‰‹æŒ‡, 2=æå‡ç‰©ä½“
        self.grasp_stage = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)
        self.approach_success = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)
        self.contact_success = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)
        self.lift_success = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)
        
        self.setup_keyboard_events()

        if "actions_mask" in self.cfg["env"]:
            self.actions_mask = torch.tensor(self.cfg["env"]["actions_mask"], device=self.device)[None, :]
        else:
            self.actions_mask = torch.ones((1, self.num_leap_hand_dofs), device=self.device)
        
        if self.debug_viz:
            self.setup_plot()

        if "debug" in self.cfg["env"]:
            self.obs_list = []
            self.target_list = []

            if "record" in self.cfg["env"]["debug"]:
                self.record_duration = int(self.cfg["env"]["debug"]["record"]["duration"] / self.control_dt)

            if "actions_file" in self.cfg["env"]["debug"]:
                self.actions_list = torch.from_numpy(np.load(self.cfg["env"]["debug"]["actions_file"])).cuda()        
                self.record_duration = self.actions_list.shape[0]
    
    def set_camera(self, position, lookat):
        """ 
        Set camera position and direction
        """

        cam_pos = gymapi.Vec3(position[0], position[1], position[2])
        cam_target = gymapi.Vec3(lookat[0], lookat[1], lookat[2])
        self.gym.viewer_camera_look_at(self.viewer, self.envs[self.lookat_id], cam_pos, cam_target)

    def lookat(self, i):
        look_at_pos = self.hand_pos[i, :].clone()
        cam_pos = look_at_pos + self.lookat_vec
        self.set_camera(cam_pos, look_at_pos)

    def render(self):
        super().render()

        if self.viewer:
            if not self.free_cam:
                self.lookat(self.lookat_id)
            
            # check for keyboard events
            for evt in self.gym.query_viewer_action_events(self.viewer):
                if not self.free_cam:
                    if evt.action == "prev_id" and evt.value > 0:
                        self.lookat_id  = (self.lookat_id-1) % self.num_envs
                        self.lookat(self.lookat_id)
                    if evt.action == "next_id" and evt.value > 0:
                        self.lookat_id  = (self.lookat_id+1) % self.num_envs
                        self.lookat(self.lookat_id)

                if evt.action == "free_cam" and evt.value > 0:
                    self.free_cam = not self.free_cam
                    
                    if self.free_cam:
                        self.gym.viewer_camera_look_at(self.viewer, None, self.default_cam_pos, self.default_cam_target)

    def setup_keyboard_events(self):
        self.lookat_id = 0
        self.free_cam = False
        self.lookat_vec = torch.tensor([0.4, -0.2, 0.1], requires_grad=False, device=self.device)

        if self.viewer is None:
            return
        
        # subscribe to keyboard shortcuts
        self.gym.subscribe_viewer_keyboard_event(
            self.viewer, gymapi.KEY_ESCAPE, "QUIT")
        self.gym.subscribe_viewer_keyboard_event(
            self.viewer, gymapi.KEY_V, "toggle_viewer_sync")
        self.gym.subscribe_viewer_keyboard_event(
            self.viewer, gymapi.KEY_F, "free_cam")
        self.gym.subscribe_viewer_keyboard_event(
            self.viewer, gymapi.KEY_LEFT_BRACKET, "prev_id")
        self.gym.subscribe_viewer_keyboard_event(
            self.viewer, gymapi.KEY_RIGHT_BRACKET, "next_id")

    def resample_randomizations(self, env_ids):
        if "joint_noise" not in self.cfg["env"]["randomization"]:
            return

        self.joint_noise_cfg = self.cfg["env"]["randomization"]["joint_noise"]

        if env_ids is None:
            self.joint_noise_iid_scale = torch.zeros((self.num_envs, self.num_leap_hand_dofs), device=self.device)
            self.joint_noise_constant_offset = torch.zeros((self.num_envs, self.num_leap_hand_dofs), device=self.device)
            self.joint_noise_outlier_scale = torch.zeros((self.num_envs, self.num_leap_hand_dofs), device=self.device)
            self.joint_noise_outlier_rate = torch.zeros((self.num_envs, self.num_leap_hand_dofs), device=self.device)
            env_ids = torch.arange(self.num_envs, device=self.device)

        if "iid" in self.joint_noise_cfg:
            low, high = self.joint_noise_cfg["iid"]["scale_range"]
            self.joint_noise_iid_scale[env_ids] = torch.rand((env_ids.shape[0], self.num_leap_hand_dofs), device=self.device) * (high - low) + low
            self.joint_noise_iid_type = self.joint_noise_cfg["iid"]["type"]

        if "constant_offset" in self.joint_noise_cfg:
            low, high = self.joint_noise_cfg["constant_offset"]["range"]
            self.joint_noise_constant_offset[env_ids] = torch.rand((env_ids.shape[0], self.num_leap_hand_dofs), device=self.device) * (high - low) + low

        if "outlier" in self.joint_noise_cfg:
            low, high = self.joint_noise_cfg["outlier"]["scale_range"]
            self.joint_noise_outlier_scale[env_ids] = torch.rand((env_ids.shape[0], self.num_leap_hand_dofs), device=self.device) * (high - low) + low
            
            low, high = self.joint_noise_cfg["outlier"]["rate_range"]
            self.joint_noise_outlier_rate[env_ids] = torch.rand((env_ids.shape[0], self.num_leap_hand_dofs), device=self.device) * (high - low) + low
            
            self.joint_noise_outlier_type = self.joint_noise_cfg["outlier"]["type"]

    def setup_plot(self):   
        self.fig, self.ax = plt.subplots()
        self.ax.set_xlim(0, 100)
        self.ax.set_ylim(-20, 20)
        self.ydata = deque(maxlen=100) # Plot 5 seconds of data
        self.ydata2 = deque(maxlen=100)
        (self.ln,) = self.ax.plot(range(len(self.ydata)), list(self.ydata), animated=True)
        (self.ln2,) = self.ax.plot(range(len(self.ydata2)), list(self.ydata2), animated=True)
        plt.show(block=False)
        plt.pause(0.1)

        self.bg = self.fig.canvas.copy_from_bbox(self.fig.bbox)
        self.ax.draw_artist(self.ln)
        self.fig.canvas.blit(self.fig.bbox)

    def set_defaults(self):
        if "include_pd_gains" not in self.cfg["env"]:
            self.cfg["env"]["include_pd_gains"] = False

        if "include_friction_coefficient" not in self.cfg["env"]:
            self.cfg["env"]["include_friction_coefficient"] = False

        if "include_obj_scales" not in self.cfg["env"]:
            self.cfg["env"]["include_obj_scales"] = False

        if "leap_hand_start_z" not in self.cfg["env"]:
            self.cfg["env"]["leap_hand_start_z"] = 0.5
        
        # ç§»é™¤ grasp_dof_search_radius - ä¸å†éœ€è¦ç¼“å­˜æœç´¢åŠå¾„
        # æ”¹ä¸ºåŸºäºé˜¶æ®µçš„æŠ“å–ç­–ç•¥é»˜è®¤é…ç½®
        if "approach_distance_threshold" not in self.cfg["env"]:
            self.cfg["env"]["approach_distance_threshold"] = 0.15

        if "contact_threshold" not in self.cfg["env"]:
            self.cfg["env"]["contact_threshold"] = 2

        if "lift_height_threshold" not in self.cfg["env"]:
            self.cfg["env"]["lift_height_threshold"] = 0.05

        if "obs_mask" not in self.cfg["env"]:
            self.cfg["env"]["obs_mask"] = None

        if "include_targets" not in self.cfg["env"]:
            self.cfg["env"]["include_targets"] = True
        
        if "include_obj_pose" not in self.cfg["env"]:
            self.cfg["env"]["include_obj_pose"] = True  # æŠ“å–ä»»åŠ¡éœ€è¦ç‰©ä½“ä½å§¿ä¿¡æ¯

        if "include_history" not in self.cfg["env"]:
            self.cfg["env"]["include_history"] = True

        if "joint_limits" not in self.cfg["env"]["randomization"]:
            self.cfg["env"]["randomization"]["joint_limits"] = 0

        if "mask_body_collision" not in self.cfg["env"]:
            self.cfg["env"]["mask_body_collision"] = {}        
    
        if "disable_actions" not in self.cfg["env"]:
            self.cfg["env"]["disable_actions"] = False

        if "disable_gravity" not in self.cfg["env"]:
            self.cfg["env"]["disable_gravity"] = False

        if "disable_object_collision" not in self.cfg["env"]:
            self.cfg["env"]["disable_object_collision"] = False

        if "disable_resets" not in self.cfg["env"]:
            self.cfg["env"]["disable_resets"] = False

        if "disable_self_collision" not in self.cfg["env"]:
            self.cfg["env"]["disable_self_collision"] = False

        # ç§»é™¤æ—‹è½¬è½´é…ç½® - åŸºäºé˜¶æ®µçš„æŠ“å–ä¸éœ€è¦æ—‹è½¬è½´
        # rotation_axis ä»…ç”¨äºæ—‹è½¬ä»»åŠ¡ï¼ŒæŠ“å–ä»»åŠ¡ä¸“æ³¨äºæ¥è¿‘ã€æ¥è§¦ã€æå‡
        # if "rotation_axis" not in self.cfg["env"]:
        #     self.rotation_axis = torch.tensor([0., 0., 1.])
        # else:
        #     self.rotation_axis = torch.tensor(self.cfg["env"]["rotation_axis"])

        # Multiple rigid shapes correspond to a rigid body, the indices can be found using get_asset_rigid_body_shape_indices
        self.body_shape_indices = [ 
            (0, 19),
            (19, 12),
            (31, 1),
            (32, 9),
            (41, 5),
            (46, 1),
            (47, 43),
            (90, 12),
            (102, 10),
            (112, 12),
            (124, 1),
            (125, 9),
            (134, 5),
            (139, 12),
            (151, 1),
            (152, 9),
            (161, 5),
            (166, 21),
            (187, 13),
            (200, 13),
            (213, 12),
            (225, 1),
        ]

    def _create_envs(self, num_envs, spacing, num_per_row):
        self._create_ground_plane()
        lower = gymapi.Vec3(-spacing, -spacing, 0.0)
        upper = gymapi.Vec3(spacing, spacing, spacing)

        self._create_object_asset()

        # set leap_hand dof properties
        self.num_leap_hand_dofs = self.gym.get_asset_dof_count(self.hand_asset)
        leap_hand_dof_props = self.gym.get_asset_dof_properties(self.hand_asset)

        self.leap_hand_dof_lower_limits = []
        self.leap_hand_dof_upper_limits = []

        for i in range(self.num_leap_hand_dofs):
            self.leap_hand_dof_lower_limits.append(leap_hand_dof_props['lower'][i])
            self.leap_hand_dof_upper_limits.append(leap_hand_dof_props['upper'][i])
            leap_hand_dof_props['effort'][i] = 0.5  #è¿™é‡Œå¾ˆæœ‰å¯èƒ½è¦ä¿®æ”¹ï¼Œå› ä¸ºæœºæ¢°è‡‚çš„æ‰­çŸ©ä¸ä¸€æ ·
            leap_hand_dof_props['stiffness'][i] = self.cfg['env']['controller']['pgain']
            leap_hand_dof_props['damping'][i] = self.cfg['env']['controller']['dgain']
            leap_hand_dof_props['friction'][i] = 0.01
            leap_hand_dof_props['armature'][i] = 0.001

        self.leap_hand_dof_lower_limits = to_torch(self.leap_hand_dof_lower_limits, device=self.device)
        self.leap_hand_dof_upper_limits = to_torch(self.leap_hand_dof_upper_limits, device=self.device)

        self.leap_hand_dof_lower_limits = self.leap_hand_dof_lower_limits.repeat((self.num_envs, 1))  
        self.leap_hand_dof_lower_limits += (2 * torch.rand_like(self.leap_hand_dof_lower_limits) - 1) * self.cfg["env"]["randomization"]["joint_limits"]
        self.leap_hand_dof_upper_limits = self.leap_hand_dof_upper_limits.repeat((self.num_envs, 1))
        self.leap_hand_dof_upper_limits += (2 * torch.rand_like(self.leap_hand_dof_upper_limits) - 1) * self.cfg["env"]["randomization"]["joint_limits"]

        hand_pose, obj_pose = self._init_object_pose()

        # compute aggregate size
        self.num_leap_hand_bodies = self.gym.get_asset_rigid_body_count(self.hand_asset)
        self.num_leap_hand_shapes = self.gym.get_asset_rigid_shape_count(self.hand_asset)
        max_agg_bodies = self.num_leap_hand_bodies + 2
        max_agg_shapes = self.num_leap_hand_shapes + 2

        self.envs = []

        self.object_init_state = []

        self.hand_indices = []
        self.object_indices = []

        leap_hand_rb_count = self.gym.get_asset_rigid_body_count(self.hand_asset)
        object_rb_count = 1
        self.object_rb_handles = list(range(leap_hand_rb_count, leap_hand_rb_count + object_rb_count))
        self.obj_scales = []
        self.object_friction_buf = torch.zeros((self.num_envs), device=self.device, dtype=torch.float)

        for i in range(num_envs):
            # create env instance
            env_ptr = self.gym.create_env(self.sim, lower, upper, num_per_row)
            if self.aggregate_mode >= 1:
                self.gym.begin_aggregate(env_ptr, max_agg_bodies * 20, max_agg_shapes * 20, True)

            # add hand - collision filter = -1 to use asset collision filters set in mjcf loader
            hand_actor = self.gym.create_actor(env_ptr, self.hand_asset, hand_pose, 'hand', i, -1, 0)
            self.gym.enable_actor_dof_force_sensors(env_ptr, hand_actor)
            self.gym.set_actor_dof_properties(env_ptr, hand_actor, leap_hand_dof_props)
            hand_idx = self.gym.get_actor_index(env_ptr, hand_actor, gymapi.DOMAIN_SIM)
            self.hand_indices.append(hand_idx)

            # add object
            object_type_id = np.random.choice(len(self.object_type_list), p=self.object_type_prob)
            object_asset = self.object_asset_list[object_type_id]

            if self.cfg["env"]["disable_object_collision"]:
                collision_group = -(i+2)
            else:
                collision_group = i

            object_handle = self.gym.create_actor(env_ptr, object_asset, obj_pose, 'object', collision_group, 0, 0)
            self.object_init_state.append([
                obj_pose.p.x, obj_pose.p.y, obj_pose.p.z,
                obj_pose.r.x, obj_pose.r.y, obj_pose.r.z, obj_pose.r.w,
                0, 0, 0, 0, 0, 0
            ])
            object_idx = self.gym.get_actor_index(env_ptr, object_handle, gymapi.DOMAIN_SIM)
            self.object_indices.append(object_idx)

            obj_scale = self.base_obj_scale
            
            if self.randomize_scale:
                num_scales = len(self.randomize_scale_list)
                obj_scale = np.random.uniform(self.randomize_scale_list[i % num_scales] - 0.025, self.randomize_scale_list[i % num_scales] + 0.025)
                
                if "randomize_scale_factor" in self.cfg["env"]:
                    obj_scale *= np.random.uniform(*self.cfg["env"]["randomize_scale_factor"])
                
                self.obj_scales.append(obj_scale)
            self.gym.set_actor_scale(env_ptr, object_handle, obj_scale)

            obj_com = [0, 0, 0]
            if self.randomize_com:
                prop = self.gym.get_actor_rigid_body_properties(env_ptr, object_handle)
                assert len(prop) == 1
                obj_com = [np.random.uniform(self.randomize_com_lower, self.randomize_com_upper),
                           np.random.uniform(self.randomize_com_lower, self.randomize_com_upper),
                           np.random.uniform(self.randomize_com_lower, self.randomize_com_upper)]
                prop[0].com.x, prop[0].com.y, prop[0].com.z = obj_com
                self.gym.set_actor_rigid_body_properties(env_ptr, object_handle, prop)

            obj_friction = 1.0
            if self.randomize_friction:
                rand_friction = np.random.uniform(self.randomize_friction_lower, self.randomize_friction_upper)
                hand_props = self.gym.get_actor_rigid_shape_properties(env_ptr, hand_actor)
                for p in hand_props:
                    p.friction = rand_friction
                self.gym.set_actor_rigid_shape_properties(env_ptr, hand_actor, hand_props)

                object_props = self.gym.get_actor_rigid_shape_properties(env_ptr, object_handle)
                for p in object_props:
                    p.friction = rand_friction
                self.gym.set_actor_rigid_shape_properties(env_ptr, object_handle, object_props)
                obj_friction = rand_friction
            self.object_friction_buf[i] = obj_friction

            if self.aggregate_mode > 0:
                self.gym.end_aggregate(env_ptr)

            self.envs.append(env_ptr)

        self.obj_scales = torch.tensor(self.obj_scales, device=self.device)
        self.object_init_state = to_torch(self.object_init_state, device=self.device, dtype=torch.float).view(self.num_envs, 13)
        self.object_rb_handles = to_torch(self.object_rb_handles, dtype=torch.long, device=self.device)
        self.hand_indices = to_torch(self.hand_indices, dtype=torch.long, device=self.device)
        self.object_indices = to_torch(self.object_indices, dtype=torch.long, device=self.device)

    def reset_idx(self, env_ids):
        """
        é‡ç½®æŒ‡å®šç¯å¢ƒçš„çŠ¶æ€ - ä½¿ç”¨åŸºäºé˜¶æ®µçš„æŠ“å–ç­–ç•¥
        ä¸å†ä¾èµ–é¢„è®¡ç®—çš„æŠ“å–å§¿æ€ç¼“å­˜
        """
        # éšæœºåŒ–ç‰©ä½“è´¨é‡ - å¯¹æŠ“å–ä»»åŠ¡å¾ˆé‡è¦ï¼Œå½±å“æŠ“å–åŠ›åº¦å’Œç¨³å®šæ€§
        if self.randomize_mass:
            lower, upper = self.randomize_mass_lower, self.randomize_mass_upper
            for env_id in env_ids:
                env = self.envs[env_id]
                handle = self.gym.find_actor_handle(env, 'object')
                prop = self.gym.get_actor_rigid_body_properties(env, handle)
                for p in prop:
                    p.mass = np.random.uniform(lower, upper)
                self.gym.set_actor_rigid_body_properties(env, handle, prop)
        # ç§»é™¤æ— ç”¨çš„ else åˆ†æ”¯ - å¦‚æœä¸éšæœºåŒ–è´¨é‡å°±ä¸éœ€è¦åšä»»ä½•æ“ä½œ

        if self.randomize_pd_gains:
            self.p_gain[env_ids] = torch_rand_float(
                self.randomize_p_gain_lower, self.randomize_p_gain_upper, (len(env_ids), self.num_actions),
                device=self.device).squeeze(1)
            self.d_gain[env_ids] = torch_rand_float(
                self.randomize_d_gain_lower, self.randomize_d_gain_upper, (len(env_ids), self.num_actions),
                device=self.device).squeeze(1)

        self.resample_randomizations(env_ids)

        # é‡ç½®åˆšä½“åŠ›
        self.rb_forces[env_ids, :, :] = 0.0

        # é‡ç½®æ‰‹éƒ¨åˆ°é»˜è®¤ä½ç½®ï¼ˆå¼€æ”¾çŠ¶æ€ï¼‰
        default_hand_pos = self.leap_hand_default_dof_pos.clone()
        self.leap_hand_dof_pos[env_ids, :] = default_hand_pos
        self.leap_hand_dof_vel[env_ids, :] = 0
        self.prev_targets[env_ids, :self.num_leap_hand_dofs] = default_hand_pos
        self.cur_targets[env_ids, :self.num_leap_hand_dofs] = default_hand_pos
        self.init_pose_buf[env_ids, :] = default_hand_pos.clone()

        # éšæœºæ”¾ç½®ç‰©ä½“ä½ç½®
        self._reset_object_poses(env_ids)

        # åº”ç”¨çŠ¶æ€æ›´æ–°
        object_indices = torch.unique(self.object_indices[env_ids]).to(torch.int32)
        self.gym.set_actor_root_state_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.root_state_tensor), 
                                                    gymtorch.unwrap_tensor(object_indices), len(object_indices))
        hand_indices = self.hand_indices[env_ids].to(torch.int32)
        self.gym.set_dof_position_target_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.prev_targets), 
                                                       gymtorch.unwrap_tensor(hand_indices), len(env_ids))
        self.gym.set_dof_state_tensor_indexed(self.sim, gymtorch.unwrap_tensor(self.dof_state), 
                                             gymtorch.unwrap_tensor(hand_indices), len(env_ids))

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        mask = self.progress_buf[env_ids] > 0
        self.object_angvel_finite_diff_ep_buf.extend(list(self.object_angvel_finite_diff_mean[env_ids][mask]))
        self.object_angvel_finite_diff_mean[env_ids] = 0

        if "print_object_angvel" in self.cfg["env"] and len(self.object_angvel_finite_diff_ep_buf) > 0:
            print("mean object angvel: ", sum(self.object_angvel_finite_diff_ep_buf) / len(self.object_angvel_finite_diff_ep_buf))

        # é‡ç½®ç¼“å†²åŒº
        self.progress_buf[env_ids] = 0
        self.obs_buf[env_ids] = 0
        self.rb_forces[env_ids] = 0
        self.at_reset_buf[env_ids] = 1
        
        # é‡ç½®åŸºäºé˜¶æ®µçš„æŠ“å–çŠ¶æ€
        self.grasp_stage[env_ids] = 0  # ä»æ¥è¿‘é˜¶æ®µå¼€å§‹
        self.approach_success[env_ids] = False
        self.contact_success[env_ids] = False
        self.lift_success[env_ids] = False

    def _reset_object_poses(self, env_ids):
        """
        éšæœºé‡ç½®ç‰©ä½“ä½ç½®å’Œæ–¹å‘ - ä¼˜åŒ–è·ç¦»æ§åˆ¶ï¼Œç¡®ä¿ç‰©ä½“åœ¨æœºæ¢°æ‰‹å¯è¾¾èŒƒå›´å†…
        """
        num_resets = len(env_ids)
        
        # åœ¨æ‰‹éƒ¨å‰æ–¹çš„åˆç†å¯è¾¾èŒƒå›´å†…éšæœºæ”¾ç½®ç‰©ä½“
        # æœºæ¢°æ‰‹ä½ç½®: (0, 0, 0.5), æ‰‹å¿ƒå‘ä¸‹
        # x: 0.05 åˆ° 0.25 ç±³ï¼ˆæ‰‹éƒ¨å‰æ–¹ï¼Œé¿å…è¿‡è¿‘å’Œè¿‡è¿œï¼‰
        # y: -0.15 åˆ° 0.15 ç±³ï¼ˆå·¦å³æ‘†åŠ¨ï¼Œå‡å°èŒƒå›´æé«˜æˆåŠŸç‡ï¼‰
        # z: 0.3 åˆ° 0.5 ç±³ï¼ˆæ‰‹éƒ¨ä¸‹æ–¹åˆ°åŒä¸€æ°´å¹³ï¼Œç¡®ä¿å¯è¾¾ï¼‰
        rand_x = torch.rand(num_resets, device=self.device) * 0.20 + 0.05  # 0.05-0.25m
        rand_y = torch.rand(num_resets, device=self.device) * 0.30 - 0.15  # -0.15-0.15m
        rand_z = torch.rand(num_resets, device=self.device) * 0.20 + 0.3   # 0.3-0.5m
        
                
        rand_rot = torch.tensor([[0, 0, 0, 1]] * num_resets, device=self.device, dtype=torch.float)

        # è®¾ç½®ç‰©ä½“ä½ç½®å’Œæ—‹è½¬
        self.root_state_tensor[self.object_indices[env_ids], 0] = rand_x
        self.root_state_tensor[self.object_indices[env_ids], 1] = rand_y
        self.root_state_tensor[self.object_indices[env_ids], 2] = rand_z
        self.root_state_tensor[self.object_indices[env_ids], 3:7] = rand_rot
        self.root_state_tensor[self.object_indices[env_ids], 7:13] = 0  # é€Ÿåº¦è®¾ä¸ºé›¶
        
        # ä¿å­˜åˆå§‹ä½ç½®ç”¨äºåç»­è®¡ç®—
        self.object_init_pose_buf[env_ids, :3] = self.root_state_tensor[self.object_indices[env_ids], :3]
        self.object_init_pose_buf[env_ids, 3:7] = self.root_state_tensor[self.object_indices[env_ids], 3:7]
        
    def get_joint_noise(self):
        tensor = torch.zeros_like(self.leap_hand_dof_pos)

        if "joint_noise" not in self.cfg["env"]["randomization"]:
            return tensor

        if not self.joint_noise_cfg["add_noise"]:
            return tensor

        if "iid" in self.joint_noise_cfg:
            if self.joint_noise_iid_type == "gaussian":
                tensor = tensor + torch.randn_like(tensor) * self.joint_noise_iid_scale
            elif self.joint_noise_iid_type == "uniform":
                tensor = tensor + (2 * torch.rand(tensor) - 1) * self.joint_noise_iid_scale
            
        if "constant_offset" in self.joint_noise_cfg:
            tensor = tensor + self.joint_noise_constant_offset

        if "outlier" in self.joint_noise_cfg:
            outlier_noise_prob = self.joint_noise_outlier_rate * self.control_dt 
            outlier_mask = torch.rand_like(outlier_noise_prob) <= outlier_noise_prob
            
            if self.joint_noise_outlier_type == "gaussian":
                tensor = tensor + torch.randn_like(tensor) * self.joint_noise_outlier_scale * outlier_mask
            elif self.joint_noise_outlier_type == "uniform":
                tensor = tensor + (2 * torch.rand(tensor) - 1) * self.joint_noise_outlier_scale * outlier_mask

        return tensor

    def compute_observations(self):
        self._refresh_gym()
        # deal with normal observation, do sliding window
        prev_obs_buf = self.obs_buf_lag_history[:, 1:].clone()
        joint_noise_matrix = self.get_joint_noise()
        cur_obs_buf = unscale(
            joint_noise_matrix.to(self.device) + self.leap_hand_dof_pos, self.leap_hand_dof_lower_limits, self.leap_hand_dof_upper_limits
        ).clone().unsqueeze(1)

        self.cur_obs_buf_noisy = cur_obs_buf.squeeze(1).clone()
        self.cur_obs_buf_clean = unscale(
            self.leap_hand_dof_pos, self.leap_hand_dof_lower_limits, self.leap_hand_dof_upper_limits
        ).clone()

        if hasattr(self, "obs_list"):
            self.obs_list.append(cur_obs_buf[0].clone())
            self.target_list.append(self.cur_targets[0].clone().squeeze())

            if self.global_counter == self.record_duration - 1:
                self.obs_list = torch.stack(self.obs_list, dim=0)
                self.obs_list = self.obs_list.cpu().numpy()

                self.target_list = torch.stack(self.target_list, dim=0)
                self.target_list = self.target_list.cpu().numpy()

                if "actions_file" in self.cfg["env"]["debug"]:
                    actions_file = os.path.basename(self.cfg["env"]["debug"]["actions_file"])
                    folder = os.path.dirname(self.cfg["env"]["debug"]["actions_file"])
                    suffix = "_".join(actions_file.split("_")[1:])
                    joints_file = os.path.join(folder, "joints_sim_{}".format(suffix)) 
                    target_file = os.path.join(folder, "targets_sim_{}".format(suffix))
                else:
                    suffix = self.cfg["env"]["debug"]["record"]["suffix"]
                    joints_file = "debug/joints_sim_{}.npy".format(suffix)
                    target_file = "debug/targets_sim_{}.npy".format(suffix)

                np.save(joints_file, self.obs_list)
                np.save(target_file, self.target_list) 
                exit()

        cur_tar_buf = self.cur_targets[:, None]
        
        if self.cfg["env"]["include_targets"]:
            cur_obs_buf = torch.cat([cur_obs_buf, cur_tar_buf], dim=-1)

        if self.cfg["env"]["include_obj_pose"]:
            cur_obs_buf = torch.cat([
                cur_obs_buf, 
                self.object_pos.unsqueeze(1)
            ], dim=-1)
        # ğŸ”§ æ–°å¢ï¼šæ‰‹éƒ¨åˆ°ç‰©ä½“çš„è·ç¦»
        if self.cfg["env"].get("include_hand_object_distance", True):
            hand_object_distance = torch.norm(self.hand_pos - self.object_pos, dim=-1)
            cur_obs_buf = torch.cat([
                cur_obs_buf,
                hand_object_distance.unsqueeze(1).unsqueeze(1)
            ], dim=-1)

        # ğŸ”§ æ–°å¢ï¼šç‰©ä½“çº¿é€Ÿåº¦ï¼ˆç”¨äºåˆ¤æ–­æŠ“å–ç¨³å®šæ€§ï¼‰
        if self.cfg["env"].get("include_object_velocity", True):
            cur_obs_buf = torch.cat([
                cur_obs_buf,
                self.object_linvel.unsqueeze(1)
            ], dim=-1)
        if self.cfg["env"]["include_obj_scales"]:
            cur_obs_buf = torch.cat([
                cur_obs_buf, 
                self.obj_scales.unsqueeze(1).unsqueeze(1), 
            ], dim=-1)
        
        if self.cfg["env"]["include_pd_gains"]:
            cur_obs_buf = torch.cat([
                cur_obs_buf, 
                self.p_gain.unsqueeze(1), 
                self.d_gain.unsqueeze(1)
            ], dim=-1)
        
        if self.cfg["env"]["include_friction_coefficient"]:
            cur_obs_buf = torch.cat([
                cur_obs_buf,
                self.object_friction_buf.unsqueeze(1).unsqueeze(1)
            ], dim=-1)

        if "phase_period" in self.cfg["env"]:
            cur_obs_buf = torch.cat([cur_obs_buf, self.phase[:, None]], dim=-1)

        if self.cfg["env"]["include_history"]:
            at_reset_env_ids = self.at_reset_buf.nonzero(as_tuple=False).squeeze(-1)
            self.obs_buf_lag_history[:] = torch.cat([prev_obs_buf, cur_obs_buf], dim=1)

            # refill the initialized buffers
            self.obs_buf_lag_history[at_reset_env_ids, :, 0:21] = unscale(
                self.leap_hand_dof_pos[at_reset_env_ids, :21], 
                self.leap_hand_dof_lower_limits[at_reset_env_ids, :21],
                self.leap_hand_dof_upper_limits[at_reset_env_ids, :21]
            ).clone().unsqueeze(1)

            if self.cfg["env"]["include_targets"]:
                self.obs_buf_lag_history[at_reset_env_ids, :, 21:42] = self.leap_hand_dof_pos[at_reset_env_ids, :21].unsqueeze(1)
            
            t_buf = (self.obs_buf_lag_history[:, -3:].reshape(self.num_envs, -1)).clone() # attach three timesteps of history

            self.obs_buf[:, :t_buf.shape[1]] = t_buf

            # self.proprio_hist_buf[:] = self.obs_buf_lag_history[:, -self.prop_hist_len:].clone()
            self.at_reset_buf[at_reset_env_ids] = 0
        else:
            self.obs_buf = cur_obs_buf.clone().squeeze(1)

        if self.cfg["env"]["obs_mask"] is not None:
            self.obs_buf = self.obs_buf * torch.tensor(self.cfg["env"]["obs_mask"], device=self.device)[None, :]

    def compute_reward(self, actions):
        self.rot_axis_buf[:, -1] = -1
        # pose diff penalty
        pose_diff_penalty = ((self.leap_hand_dof_pos - self.init_pose_buf) ** 2).sum(-1)
        # work and torque penalty
        torque_penalty = (self.torques ** 2).sum(-1)
        work_penalty = ((self.torques * self.dof_vel_finite_diff).sum(-1)) ** 2
        obj_linv_pscale = self.object_linvel_penalty_scale
        pose_diff_pscale = self.pose_diff_penalty_scale
        torque_pscale = self.torque_penalty_scale
        work_pscale = self.work_penalty_scale

        self.rew_buf[:], log_r_reward, olv_penalty = compute_hand_reward(
            self.object_linvel, obj_linv_pscale,
            self.object_angvel, self.rot_axis_buf, self.rotate_reward_scale,
            self.angvel_clip_max, self.angvel_clip_min,
            pose_diff_penalty, pose_diff_pscale,
            torque_penalty, torque_pscale,
            work_penalty, work_pscale,
        )

        if "additional_rewards" in self.cfg["env"]:
            for reward_name, reward_scale in self.cfg["env"]["additional_rewards"].items():
                reward_value = eval("self.reward_{}()".format(reward_name)) * reward_scale
                self.extras["reward_{}".format(reward_name)] = reward_value.mean()
                self.rew_buf += reward_value

        self.reset_buf[:] = self.check_termination(self.object_pos)
        
        if self.cfg["env"]["disable_resets"]:
            # only consider ep length and early termination
            self.reset_buf = self.progress_buf >= self.max_episode_length 

        self.reset_buf = self.reset_buf | self.early_termination_buf

        self.extras['rotation_reward'] = log_r_reward.mean()
        self.extras['object_linvel_penalty'] = olv_penalty.mean()
        self.extras['pose_diff_penalty'] = pose_diff_penalty.mean()
        self.extras['work_done'] = work_penalty.mean()
        self.extras['torques'] = torque_penalty.mean()
        self.extras['roll'] = self.object_angvel[:, 0].mean()
        self.extras['pitch'] = self.object_angvel[:, 1].mean()
        self.extras['yaw'] = self.object_angvel[:, 2].mean()
        self.extras['yaw_finite_diff'] = self.object_angvel_finite_diff[:, 2].mean()

        if self.evaluate:
            finished_episode_mask = self.reset_buf == 1
            self.stat_sum_rewards += self.rew_buf.sum()
            self.stat_sum_rotate_rewards += log_r_reward.sum()
            self.stat_sum_torques += self.torques.abs().sum()
            self.stat_sum_obj_linvel += (self.object_linvel ** 2).sum(-1).sum()
            self.stat_sum_episode_length += (self.reset_buf == 0).sum()
            self.env_evaluated += (self.reset_buf == 1).sum()
            self.env_timeout_counter[finished_episode_mask] += 1
            info = f'progress {self.env_evaluated} / {self.max_evaluate_envs} | ' \
                   f'reward: {self.stat_sum_rewards / self.env_evaluated:.2f} | ' \
                   f'eps length: {self.stat_sum_episode_length / self.env_evaluated:.2f} | ' \
                   f'rotate reward: {self.stat_sum_rotate_rewards / self.env_evaluated:.2f} | ' \
                   f'lin vel (x100): {self.stat_sum_obj_linvel * 100 / self.stat_sum_episode_length:.4f} | ' \
                   f'command torque: {self.stat_sum_torques / self.stat_sum_episode_length:.2f}'
            if self.env_evaluated >= self.max_evaluate_envs:
                exit()
    
    def post_physics_step(self):
        self.progress_buf += 1
        self.reset_buf[:] = 0
        self.early_termination_buf[:] = 0
        self._refresh_gym()
        self.compute_reward(self.actions)
        env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)
        if len(env_ids) > 0:
            self.reset_idx(env_ids)
        self.compute_observations()

        if self.viewer and self.debug_viz:
            # draw axes on target object
            self.gym.clear_lines(self.viewer)
            self.gym.refresh_rigid_body_state_tensor(self.sim)

            for i in range(self.num_envs):
                objectx = (self.object_pos[i] + quat_apply(self.object_rot[i], to_torch([1, 0, 0], device=self.device) * 0.2)).cpu().numpy()
                objecty = (self.object_pos[i] + quat_apply(self.object_rot[i], to_torch([0, 1, 0], device=self.device) * 0.2)).cpu().numpy()
                objectz = (self.object_pos[i] + quat_apply(self.object_rot[i], to_torch([0, 0, 1], device=self.device) * 0.2)).cpu().numpy()

                p0 = self.object_pos[i].cpu().numpy()
                self.gym.add_lines(self.viewer, self.envs[i], 1, [p0[0], p0[1], p0[2], objectx[0], objectx[1], objectx[2]], [0.85, 0.1, 0.1])
                self.gym.add_lines(self.viewer, self.envs[i], 1, [p0[0], p0[1], p0[2], objecty[0], objecty[1], objecty[2]], [0.1, 0.85, 0.1])
                self.gym.add_lines(self.viewer, self.envs[i], 1, [p0[0], p0[1], p0[2], objectz[0], objectz[1], objectz[2]], [0.1, 0.1, 0.85])
                
            self.plot_callback()

    def plot_callback(self):
        self.fig.canvas.restore_region(self.bg)

        # self.ydata.append(self.object_rpy[0, 2].item())
        self.ydata.append(self.object_angvel_finite_diff[0, 2].item())
        self.ydata2.append(self.object_rpy[0, 2].item())

        self.ln.set_ydata(list(self.ydata))
        self.ln.set_xdata(range(len(self.ydata)))

        self.ln2.set_ydata(list(self.ydata2))
        self.ln2.set_xdata(range(len(self.ydata2)))

        self.ax.draw_artist(self.ln)
        self.ax.draw_artist(self.ln2)
        self.fig.canvas.blit(self.fig.bbox)
        self.fig.canvas.flush_events()

    def _create_ground_plane(self):
        plane_params = gymapi.PlaneParams()
        plane_params.normal = gymapi.Vec3(0.0, 0.0, 1.0)
        self.gym.add_ground(self.sim, plane_params)

    def pre_physics_step(self, actions):
        self.global_counter += 1
        
        if hasattr(self, "actions_list"):
            actions = self.actions_list[self.global_counter-1].repeat((self.num_envs, 1))

        actions = torch.clamp(actions, -1.0, 1.0)
        self.actions = actions.clone().to(self.device)
        self.actions *= self.actions_mask

        targets = self.prev_targets + 1 / 24 * self.actions
        self.cur_targets[:] = tensor_clamp(targets, self.leap_hand_dof_lower_limits, self.leap_hand_dof_upper_limits)
        
        # Code for debugging joint angles
        # self.cur_targets = torch.zeros_like(self.cur_targets)
        # self.cur_targets[:, 5] = math.sin(self.global_counter / 20 * 2 * math.pi / 2)
        # self.cur_targets = scale(self.cur_targets, self.leap_hand_dof_upper_limits, self.leap_hand_dof_lower_limits)
        
        self.prev_targets[:] = self.cur_targets.clone()

        if self.force_scale > 0.0:
            self.rb_forces *= torch.pow(self.force_decay, self.dt / self.force_decay_interval)
            # apply new forces
            obj_mass = to_torch(
                [self.gym.get_actor_rigid_body_properties(env, self.gym.find_actor_handle(env, 'object'))[0].mass for
                 env in self.envs], device=self.device)
            prob = self.random_force_prob_scalar
            force_indices = (torch.less(torch.rand(self.num_envs, device=self.device), prob)).nonzero()
            self.rb_forces[force_indices, self.object_rb_handles, :] = torch.randn(
                self.rb_forces[force_indices, self.object_rb_handles, :].shape,
                device=self.device) * obj_mass[force_indices, None] * self.force_scale
            self.gym.apply_rigid_body_force_tensors(self.sim, gymtorch.unwrap_tensor(self.rb_forces), None, gymapi.ENV_SPACE)

    def reset(self):
        super().reset()
        return self.obs_dict
    
    def construct_sim_to_real_transformation(self):
        self.sim_dof_order = self.gym.get_actor_dof_names(self.envs[0], 0)
        self.sim_dof_order = [int(x) for x in self.sim_dof_order]
        self.real_dof_order = list(range(21))
        self.sim_to_real_indices = [] # Value at i is the location of ith real index in the sim list

        for x in self.real_dof_order:
            self.sim_to_real_indices.append(self.sim_dof_order.index(x))
        
        self.real_to_sim_indices = []

        for x in self.sim_dof_order:
            self.real_to_sim_indices.append(self.real_dof_order.index(x))
        
        import pdb; pdb.set_trace()
        assert(self.sim_to_real_indices == self.cfg["env"]["sim_to_real_indices"])
        assert(self.real_to_sim_indices == self.cfg["env"]["real_to_sim_indices"])

    def real_to_sim(self, values):
        if not hasattr(self, "sim_dof_order"):
            self.construct_sim_to_real_transformation()

        return values[:, self.real_to_sim_indices]

    def sim_to_real(self, values):
        if not hasattr(self, "sim_dof_order"):
            self.construct_sim_to_real_transformation()
        
        return values[:, self.sim_to_real_indices]

    def update_low_level_control(self):
        previous_dof_pos = self.leap_hand_dof_pos.clone()
        self._refresh_gym()      
        if os.getenv("RVIZ") is None and not self.cfg["env"]["disable_actions"]:
            self.gym.set_dof_position_target_tensor(self.sim, gymtorch.unwrap_tensor(self.cur_targets))

    def check_termination(self, object_pos):
        resets = torch.logical_or(
            #torch.greater(object_pos[:, -1], ),
            torch.less(object_pos[:, -1], self.reset_z_threshold),
            torch.greater_equal(self.progress_buf, self.max_episode_length),
        )

        return resets

    def _refresh_gym(self):
        self.gym.refresh_dof_state_tensor(self.sim)
        self.gym.refresh_actor_root_state_tensor(self.sim)
        self.gym.refresh_rigid_body_state_tensor(self.sim)
        self.gym.refresh_dof_force_tensor(self.sim)
        self.gym.refresh_net_contact_force_tensor(self.sim)
        self.object_pose = self.root_state_tensor[self.object_indices, 0:7]
        self.object_pos = self.root_state_tensor[self.object_indices, 0:3]
        self.object_rot = self.root_state_tensor[self.object_indices, 3:7]
        self.hand_pos = self.root_state_tensor[self.hand_indices, 0:3]
        self.object_linvel = self.root_state_tensor[self.object_indices, 7:10]
        self.object_angvel = self.root_state_tensor[self.object_indices, 10:13]

        if self.prev_global_counter != self.global_counter: # This is required since sometimes _refresh_gym is called multiple times within same step
            new_object_roll, new_object_pitch, new_object_yaw = euler_from_quaternion(self.object_rot)
            new_object_rpy = torch.stack((new_object_roll, new_object_pitch, new_object_yaw), dim=1) 
            delta_counter = self.global_counter - self.prev_global_counter
            self.object_rpy = new_object_rpy
            self.prev_global_counter = self.global_counter
            
            dr, dp, dy = euler_from_quaternion(quat_mul(self.object_rot, quat_conjugate(self.previous_object_rot)))
            self.object_angvel_finite_diff = torch.stack([dr, dp, dy], dim=-1)
            self.object_angvel_finite_diff /= (self.control_dt * delta_counter)
            self.previous_object_rot = self.object_rot.clone() 

        if "phase_period" in self.cfg["env"]:
            omega = 2 * math.pi / self.cfg["env"]["phase_period"]
            phase_angle = (self.progress_buf - 1) * self.control_dt * omega
            self.phase = torch.stack([torch.sin(phase_angle), torch.cos(phase_angle)], dim=-1)

    def _setup_domain_rand_cfg(self, rand_cfg):
        self.randomize_mass = rand_cfg['randomizeMass']
        self.randomize_mass_lower = rand_cfg['randomizeMassLower']
        self.randomize_mass_upper = rand_cfg['randomizeMassUpper']
        self.randomize_com = rand_cfg['randomizeCOM']
        self.randomize_com_lower = rand_cfg['randomizeCOMLower']
        self.randomize_com_upper = rand_cfg['randomizeCOMUpper']
        self.randomize_friction = rand_cfg['randomizeFriction']
        self.randomize_friction_lower = rand_cfg['randomizeFrictionLower']
        self.randomize_friction_upper = rand_cfg['randomizeFrictionUpper']
        self.randomize_scale = rand_cfg['randomizeScale']
        self.scale_list_init = rand_cfg['scaleListInit']
        self.randomize_scale_list = rand_cfg['randomizeScaleList']
        self.randomize_scale_lower = rand_cfg['randomizeScaleLower']
        self.randomize_scale_upper = rand_cfg['randomizeScaleUpper']
        self.randomize_pd_gains = rand_cfg['randomizePDGains']
        self.randomize_p_gain_lower = rand_cfg['randomizePGainLower']
        self.randomize_p_gain_upper = rand_cfg['randomizePGainUpper']
        self.randomize_d_gain_lower = rand_cfg['randomizeDGainLower']
        self.randomize_d_gain_upper = rand_cfg['randomizeDGainUpper']

    def _setup_priv_option_cfg(self, p_cfg):
        self.enable_priv_obj_position = p_cfg['enableObjPos']
        self.enable_priv_obj_mass = p_cfg['enableObjMass']
        self.enable_priv_obj_scale = p_cfg['enableObjScale']
        self.enable_priv_obj_com = p_cfg['enableObjCOM']
        self.enable_priv_obj_friction = p_cfg['enableObjFriction']

    def _setup_object_info(self, o_cfg):
        self.object_type = o_cfg['type']
        raw_prob = o_cfg['sampleProb']
        assert (sum(raw_prob) == 1)

        primitive_list = self.object_type.split('+')
        print('---- Primitive List ----')
        print(primitive_list)
        self.object_type_prob = []
        self.object_type_list = []
        self.asset_files_dict = {
            'simple_tennis_ball': 'assets/ball.urdf',
            'cube': 'assets/cube.urdf'
        }
        for p_id, prim in enumerate(primitive_list):
            if 'cuboid' in prim:
                subset_name = self.object_type.split('_')[-1]
                cuboids = sorted(glob(f'../assets/cuboid/{subset_name}/*.urdf'))
                cuboid_list = [f'cuboid_{i}' for i in range(len(cuboids))]
                self.object_type_list += cuboid_list
                for i, name in enumerate(cuboids):
                    self.asset_files_dict[f'cuboid_{i}'] = name.replace('../assets/', 'assets/')
                self.object_type_prob += [raw_prob[p_id] / len(cuboid_list) for _ in cuboid_list]
            elif 'cylinder' in prim:
                subset_name = self.object_type.split('_')[-1]
                cylinders = sorted(glob(f'../assets/cylinder/{subset_name}/*.urdf'))
                cylinder_list = [f'cylinder_{i}' for i in range(len(cylinders))]
                self.object_type_list += cylinder_list
                for i, name in enumerate(cylinders):
                    self.asset_files_dict[f'cylinder_{i}'] = name.replace('../assets/', 'assets/')
                self.object_type_prob += [raw_prob[p_id] / len(cylinder_list) for _ in cylinder_list]
            else:
                self.object_type_list += [prim]
                self.object_type_prob += [raw_prob[p_id]]
        print('---- Object List ----')
        print(self.object_type_list)
        assert (len(self.object_type_list) == len(self.object_type_prob))

    def _setup_reward_cfg(self, r_cfg):
        self.angvel_clip_min = r_cfg['angvelClipMin']
        self.angvel_clip_max = r_cfg['angvelClipMax']
        self.rotate_reward_scale = r_cfg['rotateRewardScale']
        self.object_linvel_penalty_scale = r_cfg['objLinvelPenaltyScale']
        self.pose_diff_penalty_scale = r_cfg['poseDiffPenaltyScale']
        self.torque_penalty_scale = r_cfg['torquePenaltyScale']
        self.work_penalty_scale = r_cfg['workPenaltyScale']

    def _create_object_asset(self):
        # object file to asset
        asset_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
        hand_asset_file = self.cfg['env']['asset']['handAsset']
        # load hand asset
        hand_asset_options = gymapi.AssetOptions()
        hand_asset_options.flip_visual_attachments = False
        hand_asset_options.fix_base_link = True
        hand_asset_options.collapse_fixed_joints = True
        hand_asset_options.disable_gravity = False
        hand_asset_options.thickness = 0.001
        hand_asset_options.angular_damping = 0.01

        # Convex decomposition
        hand_asset_options.vhacd_enabled = True
        hand_asset_options.vhacd_params.resolution = 300000
        # hand_asset_options.vhacd_params.max_convex_hulls = 30
        # hand_asset_options.vhacd_params.max_num_vertices_per_ch = 64

        hand_asset_options.default_dof_drive_mode = gymapi.DOF_MODE_POS
        self.hand_asset = self.gym.load_asset(self.sim, asset_root, hand_asset_file, hand_asset_options)
        
        if "leap_hand" in hand_asset_file:
            rsp = self.gym.get_asset_rigid_shape_properties(self.hand_asset)   

            for i, (_, body_group) in enumerate(self.cfg["env"]["mask_body_collision"].items()):
                filter_value = 2 ** i

                for body_idx in body_group:
                    start, count = self.body_shape_indices[body_idx]
                    
                    for idx in range(count):
                        rsp[idx + start].filter = rsp[idx + start].filter | filter_value 

            if self.cfg["env"]["disable_self_collision"]: # Disable all collisions
                for i in range(len(rsp)):
                    rsp[i].filter = 1

            self.gym.set_asset_rigid_shape_properties(self.hand_asset, rsp)

        # load object asset
        self.object_asset_list = []
        for object_type in self.object_type_list:
            object_asset_file = self.asset_files_dict[object_type]
            object_asset_options = gymapi.AssetOptions()

            if self.cfg["env"]["disable_gravity"]:
                object_asset_options.disable_gravity = True

            object_asset = self.gym.load_asset(self.sim, asset_root, object_asset_file, object_asset_options)
            self.object_asset_list.append(object_asset)

    def _init_object_pose(self):
        leap_hand_start_pose = gymapi.Transform()
        leap_hand_start_pose.p = gymapi.Vec3(0, 0, self.cfg["env"]["leap_hand_start_z"])

        leap_hand_start_pose.r = gymapi.Quat.from_axis_angle(gymapi.Vec3(1, 0, 0), np.pi) 
        object_start_pose = gymapi.Transform()
        object_start_pose.p = gymapi.Vec3()
        object_start_pose.p.x = leap_hand_start_pose.p.x
        pose_dx, pose_dy, pose_dz = -0.01, -0.03, 0.15

        if "override_object_init_x" in self.cfg["env"]:
            pose_dx = self.cfg["env"]["override_object_init_x"]

        if "override_object_init_y" in self.cfg["env"]:
            pose_dy = self.cfg["env"]["override_object_init_y"]

        object_start_pose.p.x = leap_hand_start_pose.p.x + pose_dx
        object_start_pose.p.y = leap_hand_start_pose.p.y + pose_dy
        object_start_pose.p.z = leap_hand_start_pose.p.z + pose_dz

        # ç§»é™¤ç¼“å­˜ç›¸å…³çš„å¯¹è±¡åˆå§‹ä½ç½®é€»è¾‘
        # ç°åœ¨ç»Ÿä¸€ä½¿ç”¨é»˜è®¤çš„ç‰©ä½“é«˜åº¦ä½ç½®
        object_z = 0.64  # ä½¿ç”¨å›ºå®šçš„é»˜è®¤é«˜åº¦
        object_start_pose.p.z = object_z

        if "override_object_init_z" in self.cfg["env"]:
            object_start_pose.p.z = self.cfg["env"]["override_object_init_z"] 

        return leap_hand_start_pose, object_start_pose

    def reward_rotate_finite_diff(self):
        min_angvel = self.cfg["env"]["reward"]["angvelClipMin"]
        max_angvel = self.cfg["env"]["reward"]["angvelClipMax"]

        reward = torch.clip(self.object_angvel_finite_diff[:, 2], min=min_angvel, max=max_angvel)

        N = self.progress_buf  
        self.object_angvel_finite_diff_mean = N * self.object_angvel_finite_diff_mean / (N  + 1) + reward / (N + 1) 

        return reward

    def reward_object_fallen(self):
        return torch.less(self.object_pos[:, -1], self.reset_z_threshold).float()

    def LEAPsim_limits(self):
        sim_min = self.sim_to_real(self.leap_hand_dof_lower_limits).squeeze().cpu().numpy()
        sim_max = self.sim_to_real(self.leap_hand_dof_upper_limits).squeeze().cpu().numpy()
        
        return sim_min, sim_max

    def LEAPhand_to_sim_ones(self, joints):
        joints = self.LEAPhand_to_LEAPsim(joints)
        sim_min, sim_max = self.LEAPsim_limits()
        joints = unscale_np(joints, sim_min, sim_max)

        return joints
    
    def LEAPhand_to_LEAPsim(self, joints):
        joints = np.array(joints)
        ret_joints = joints - 3.14159
        
        return ret_joints

def compute_hand_reward(
    object_linvel, object_linvel_penalty_scale: float,
    object_angvel, rotation_axis, rotate_reward_scale: float,
    angvel_clip_max: float, angvel_clip_min: float,
    pose_diff_penalty, pose_diff_penalty_scale: float,
    torque_penalty, torque_pscale: float,
    work_penalty, work_pscale: float,
):
    rotate_reward_cond = (rotation_axis[:, -1] != 0).float()
    vec_dot = (object_angvel * rotation_axis).sum(-1)
    rotate_reward = torch.clip(vec_dot, max=angvel_clip_max, min=angvel_clip_min)
    rotate_reward = rotate_reward_scale * rotate_reward * rotate_reward_cond
    object_linvel_penalty = torch.norm(object_linvel, p=1, dim=-1)

    reward = rotate_reward
    # Distance from the hand to the object
    reward = reward + object_linvel_penalty * object_linvel_penalty_scale
    reward = reward + pose_diff_penalty * pose_diff_penalty_scale
    reward = reward + torque_penalty * torque_pscale
    reward = reward + work_penalty * work_pscale
    return reward, rotate_reward, object_linvel_penalty

def euler_from_quaternion(quat_angle):
    """
    Convert a quaternion into euler angles (roll, pitch, yaw)
    roll is rotation around x in radians (counterclockwise)
    pitch is rotation around y in radians (counterclockwise)
    yaw is rotation around z in radians (counterclockwise)
    """
    x = quat_angle[:,0]; y = quat_angle[:,1]; z = quat_angle[:,2]; w = quat_angle[:,3]
    t0 = +2.0 * (w * x + y * z)
    t1 = +1.0 - 2.0 * (x * x + y * y)
    roll_x = torch.atan2(t0, t1)
    
    t2 = +2.0 * (w * y - z * x)
    t2 = torch.clip(t2, -1, 1)
    pitch_y = torch.asin(t2)
    
    t3 = +2.0 * (w * z + x * y)
    t4 = +1.0 - 2.0 * (y * y + z * z)
    yaw_z = torch.atan2(t3, t4)
    
    return roll_x, pitch_y, yaw_z # in radians

def unscale_np(x, lower, upper):
    return (2.0 * x - upper - lower)/(upper - lower)

def quat_rotate_inverse(q, v):
    shape = q.shape
    q_w = q[:, -1]
    q_vec = q[:, :3]
    a = v * (2.0 * q_w ** 2 - 1.0).unsqueeze(-1)
    b = torch.cross(q_vec, v, dim=-1) * q_w.unsqueeze(-1) * 2.0
    c = q_vec * \
        torch.bmm(q_vec.view(shape[0], 1, 3), v.view(
            shape[0], 3, 1)).squeeze(-1) * 2.0
    return a - b + c
